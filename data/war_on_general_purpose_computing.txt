How Laws Restricting Tech Actually Expose Us to Greater Harm

We live in a world made of computers. Your car is a computer that
drives down the freeway at 60 mph with you strapped inside. If you live
or work in a modern building, computers regulate its temperature and
respiration. And we’re not just putting our bodies inside
computers—we’re also putting computers inside our bodies. I recently
exchanged words in an airport lounge with a late arrival who wanted to
use the sole electrical plug, which I had beat him to, fair and square.
“I need to charge my laptop,” I said. “I need to charge my leg,” he
said, rolling up his pants to show me his robotic prosthesis. I
surrendered the plug.

You and I and everyone who grew up with earbuds? There’s a day in our
future when we’ll have hearing aids, and chances are they won’t be
retro-hipster beige transistorized analog devices: They’ll be computers
in our heads.

And that’s why the current regulatory paradigm for computers, inherited
from the 16-year-old stupidity that is the Digital Millennium Copyright
Act, needs to change. As things stand, the law requires that computing
devices be designed to sometimes disobey their owners, so that their
owners won’t do something undesirable. To make this work, we also have
to criminalize anything that might help owners change their computers
to let the machines do that supposedly undesirable thing.

This approach to controlling digital devices was annoying back in, say,
1995, when we got the DVD player that prevented us from skipping ads or
playing an out-of-region disc. But it will be intolerable and deadly
dangerous when our 3-D printers, self-driving cars, smart houses, and
even parts of our bodies are designed with the same restrictions.
Because those restrictions would change the fundamental nature of
computers. Speaking in my capacity as a dystopian science fiction
writer: This scares the hell out of me.

If we are allowed to have total control over our own computers, we may
enter a sci-fi world of unparalleled leisure and excitement.

The general-purpose computer is one of the crowning achievements of
industrial society. Prior to its invention, electronic calculating
engines were each hardwired to do just one thing, like calculate
ballistics tables. John von Neumann’s “von Neumann architecture” and
Alan Turing’s “Turing-complete computer” provided the theoretical basis
for building a calculating engine that could run any program that could
be expressed in symbolic language. That breakthrough still ripples
through society, revolutionizing every corner of our world. When
everything is made of computers, an improvement in computers makes
everything better.

But there’s a terrible corollary to that virtuous cycle: Any law or
regulation that undermines computers’ utility or security also ripples
through all the systems that have been colonized by the general-purpose
computer. And therein lies the potential for untold trouble and
mischief.

Because while we’ve spent the past 70 years perfecting the art of
building computers that can run every single program, we have no idea
how to build a computer that can run every program except the one that
infringes copyright or prints out guns or lets a software-based radio
be used to confound air-traffic control signals or cranks up the
air-conditioning even when the power company sends a peak-load message
to it.

The closest approximation we have for “a computer that runs all the
programs except the one you don’t like” is “a computer that is infected
with spyware out of the box.” By spyware I mean operating-system
features that monitor the computer owner’s commands and cancel them if
they’re on a blacklist. Think, for example, of image scanners that can
detect if you’re trying to scan currency and refuse to further process
the image. As much as we want to prevent counterfeiting, imposing codes
and commands that you can’t overrule is a recipe for disaster.

Why? Because for such a system to work, remote parties must have more
privileges on it than the owner. And such a security model must hide
its operation from the computer’s normal processes. When you ask your
computer to do something reasonable, you expect it to say, “Yes,
master” (or possibly “Are you sure?”), not “I CAN’T LET YOU DO THAT,
DAVE.”

If the “I CAN’T LET YOU DO THAT, DAVE” message is being generated by a
program on your desktop labeled HAL9000.exe, you will certainly drag
that program into the trash. If your computer’s list of running
programs shows HAL9000.exe lurking in the background like an
immigration agent prowling an arrivals hall, looking for sneaky cell
phone users to shout at, you will terminate that process with a
satisfied click.

So the only way to sustain HAL9000.exe and its brethren—the programs
that today keep you from installing non-App Store apps on your iPhone
and tomorrow will try to stop you from printing gun.stl on your 3-D
printer—is to design the computer to hide them from you. And that
creates vulnerabilities that make your computer susceptible to
malicious hacking. Consider what happened in 2005, when Sony BMG
started selling CDs laden with the notorious Sony rootkit, software
designed to covertly prevent people from copying music files. Once you
put one of Sony BMG’s discs into your computer’s CD drive, it would
change your OS so that files beginning with $sys$ were invisible to the
system. The CD then installed spyware that watched for attempts to rip
any music CD and silently blocked them. Of course, virus writers
quickly understood that millions of PCs were now blind to any file that
began with $sys$ and changed the names of their viruses accordingly,
putting legions of computers at risk.

Code always has flaws, and those flaws are easy for bad guys to find.
But if your computer has deliberately been designed with a blind spot,
the bad guys will use it to evade detection by you and your antivirus
software. That’s why a 3-D printer with anti-gun-printing code isn’t a
3-D printer that won’t print guns—the bad guys will quickly find a way
around that. It’s a 3-D printer that is vulnerable to hacking by
malware creeps who can use your printer’s “security” against you: from
bricking your printer to screwing up your prints to introducing subtle
structural flaws to simply hijacking the operating system and using it
to stage attacks on your whole network.

This business of designing computers to deliberately weasel and lie
isn’t the worst thing about the war on the general-purpose computer and
the effort to bodge together a “Turing-almost-complete” architecture
that can run every program except for one that distresses a government,
police force, corporation, or spy agency.

No, the worst part is that, like the lady who had to swallow the bird
to catch the spider that she’d swallowed to catch the fly, any
technical system that stops you from being the master of your computer
must be accompanied by laws that criminalize information about its
weaknesses. In the age of Google, it simply won’t do to have “uninstall
HAL9000.exe” return a list of videos explaining how to jailbreak your
gadgets, just as videos that explain how to jailbreak your iPhone today
could technically be illegal; making and posting them could potentially
put their producers (and the sites that host them) at risk of
prosecution.

This amounts to a criminal sanction for telling people about
vulnerabilities in their own computers. And because today your computer
lives in your pocket and has a camera and a microphone and knows all
the places you go; and because tomorrow that speeding car/computer
probably won’t even sport a handbrake, let alone a steering wheel—the
need to know about any mode that could be exploited by malicious
hackers will only get more urgent. There can be no “lawful
interception” capacity for a self-driving car, allowing police to order
it to pull over, that wouldn’t also let a carjacker compromise your car
and drive it to a convenient place to rob, rape, and/or kill you.

If those million-eyed, fast-moving, deep-seated computers are designed
to obey their owners; if the policy regulating those computers
encourages disclosure of flaws, even if they can be exploited by spies,
criminals, and cops; if we’re allowed to know how they’re configured
and permitted to reconfigure them without being overridden by a distant
party—then we may enter a science fictional world of unparalleled
leisure and excitement.

But if the world’s governments continue to insist that wiretapping
capacity must be built into every computer; if the state of California
continues to insist that cell phones have kill switches allowing remote
instructions to be executed on your phone that you can’t countermand or
even know about; if the entertainment industry continues to insist that
the general-purpose computer must be neutered so you can’t use it to
watch TV the wrong way; if the World Wide Web Consortium continues to
infect the core standards of the web itself to allow remote control
over your computer against your wishes—then we are in deep, deep
trouble.

The Internet isn’t just the world’s most perfect video-on-demand
service. It’s not simply a better way to get pornography. It’s not
merely a tool for planning terrorist attacks. Those are only use cases
for the net; what the net is, is the nervous system of the 21st
century. It’s time we started acting like it.

There can be no doubt that general-purpose computing has been a boon to
the world. The ability to run different kinds of programs, from various
sources, including bought from companies, written from scratch, and,
well, built from source, is something that we take for granted on
many—most—of the computing devices that we own. But that model seems to
be increasingly disappearing in many kinds of devices, including
personal computers, as a recent kerfluffle in the Apple world helps to
demonstrate.

In mid-November, macOS users suddenly started having difficulty
launching applications on their systems. It was taking minutes to
launch applications and the timing seemed suspiciously aligned with the
release of macOS "Big Sur" on the same day. It turned out that Apple's
Online Certificate Status Protocol (OCSP) servers were overwhelmed or
otherwise non-functional, which led to the problems.

OCSP is used as part of the process of verifying notarized applications
on macOS; those applications are signed by the developer's key. Apple
signs the developer's public key, which is contained in a certificate
similar to those used by TLS, but the system needs to check to ensure
that the key has not been revoked. This check is performed at
installation time and then each time the application is run.

Normally, if the OCSP servers are not available, because they are down
or the system is not connected to the internet, the connection will
fail, which is treated as a "soft failure" so the certificate is
considered valid. That way, the applications open immediately. During
the outage, though, the servers were up but not responding correctly,
so the applications would not launch until the connection timed out.
That raised the visibility of the OCSP checking, which had already been
going on in macOS for some time.

The failure led to a rather over-the-top blog post by Jeffrey Paul that
pointed out some major privacy flaws with OCSP, especially in relation
to the checking that macOS Gatekeeper does to ensure that applications
have valid signatures before running them. Every time an
internet-connected macOS system starts an application, an OCSP query
with a whole treasure trove of private information is sent to Apple.
Obviously, the servers know what date and time the request was made and
the IP address from which it was made; the latter greatly narrows down
the geographic location of the system in question. There is also a hash
sent for the certificate being queried, which Paul inaccurately called
the "application hash". All of that gives Apple a bunch of data that
folks may not really want to provide to the company, but the OCSP
queries are made over unencrypted HTTP. So anyone able to see the
traffic (e.g. ISPs, government spy agencies, WiFi hotspot providers)
also gets a look at which applications the user is running, when they
are running them, and where.

Paul's analysis was somewhat flawed (as pointed out by Jacopo Jannone
and probably others) in that the hash being sent is for the developer
certificate, not the application itself. In many cases, that may amount
to the same thing because the developers only ship a single
application, but the OCSP check is not directly sending a hash that
uniquely identifies the application. The information it does send is
still pretty useful to Apple, its content-delivery network (CDN)
partner Akamai, and any "man in the middle" that can see the traffic.
There are also the US and other governments to consider, which can (and
do) regularly request records of this sort, without needing a warrant.

In some sense, the privacy implications are not all that different from
those of web browsers, which also use OCSP to determine if the TLS
certificates for HTTPS sites have been revoked. They use OCSP
unencrypted as well, since there is something of a chicken-and-egg
problem with regard to determining certificate validity over HTTPS;
OCSP stapling helps get around that problem to some extent. There is
also a big difference, though, in that all of the macOS application
OCSP checks go to Apple servers, while the checks for
web-site-certificate validity go to various certificate authorities
(CAs) rather than a central location.

In response to the uproar from the OCSP server failures, Paul's post,
Twitter threads like this one from Jeff Johnson, and more, Apple
published a statement about its security checks. As part of that, the
company committed to stop collecting IP addresses from the OCSP checks,
to create a new encrypted protocol for doing the checks, and to provide
a way for users to opt out of the checks.

In general, checking the integrity of programs before running them is a
good thing, of course, but the devil is in the details. Some strongly
believe that Apple is making the right decisions to secure its users
and their computers, but there is a cost to that security. Apple
effectively gets to decide which programs can be installed and run on
macOS systems, at least for most users. If a developer crosses the
company in some way, their certificate can be revoked. If the developer
does not want to ask Apple's permission (in the form of a developer
certificate), their applications cannot be installed and run at all.

There are, evidently, settings and mechanisms to get around these
checks now, which is something of a double-edged sword. On one hand, it
restores control of the system to owner of the hardware, but on the
other, it opens said owner up to a number of unpleasant risks. On the
gripping hand, perhaps, is the concern that those mechanisms may be
disappearing over time. Do triple-edged swords exist?

For example, a tool called Little Snitch has been used to bypass the
OCSP checking, but it no longer works for that purpose in Big Sur.
Apple has exempted some of its applications from being routed through
the frameworks that third-party firewall programs (like Little Snitch)
must use. That allows those programs to evade the firewalls and even
bypass VPNs. That seems like a good way to wrest control from the
owners of the hardware, in truth.

Potentially worse still is the lockdown that may be coming on the new
Apple Arm-based hardware. Every other Arm-based Apple device (e.g.
iPhone, iPad) is locked down such that only Apple-approved operating
systems can be installed on it. Those new macOS systems will only run
the Big Sur version, so their owners effectively cannot control their
network traffic via firewalls or VPNs. If Apple stays the course with
lockdowns for Arm-based hardware, it is a little hard to see what,
exactly, buyers of that hardware are "owning".

In a Twitter thread, Ruby on Rails creator Dave Heinemeier Hansson
summed the problem up nicely:
The whole process of having Apple mix these “protections against
malware” into a system that’s also a “protection of our business model”
remains deeply problematic. Apple is clearly positioning the App Store
to soon be the only “trusted” default. Locking the Mac as with iOS.

We need to remain vigilant, and resist these power grabs masquerading
purely as benevolent security measures. Yes, there are security
benefits. No, we don’t trust Apple to dictate whether our computers
should be allowed to run a piece of software. We already lost that on
iOS.

Free software is a way out of that future, for sure, but it requires
being able to install and run software of the owner's choice on their
devices. That may also be less secure in some scenarios, but it is
clearly more free (as in freedom). That makes sense for those who are
technically savvy, but sadly may leave our less knowledgeable friends,
neighbors, and family behind. Protecting those folks, without forcing
them to buy into a walled garden of one sort or another, is a difficult
nut to crack.

The Internet Of Someone Else’s Things

The Internet Of Things is coming. Rejoice! …Mostly. It will open our
collective eyes to petabytes of real-time data, which we will turn into
new insights and efficiencies. It will doubtless save lives. Oh, yes:
and it will subtly redefine ownership as we know it. You will no longer
own many of the most expensive and sophisticated items you possess. You
may think you own them. But you’ll be wrong.

They say “possession is nine-tenths of the law,” but even if you
physically and legally own a Smart Thing, you won’t actually control
it. Ownership will become a three-legged stool: who physically owns a
thing; who legally owns it; …and who has the ultimate power to command
it. Who, in short, has root.

This is not a hypothetical situation. Your phone probably has three
separate computers in it (processor, baseband processor, and SIM card)
and you almost certainly don’t have root on any of them, which is why
some people refer to phones as “tracking devices which make phone
calls.” The New York Times recently ran a story about cars being
prevented from starting because payments were days late. (And as
CityLab points out: “Losing transportation could mean losing
everything.”) Consider also the recent discovery that Belkin routers
apparently had to connect to Belkin’s servers before they would connect
to the rest of the Internet.

As The Atlantic puts it:

"The smarter one’s things, the greater the possibility that they’ll
be conscripted into schemes you never would have imagined and might
not like."

The fundamental issue here is that the Internet of Things will not have
a standard set of open APIs for consumers. (Well, there’s ThingSpeak,
but it’s not exactly widely supported.) You can’t get your Tesla to
dump all of its data to a server you specify. While Nest has a public
API, they maintain gatekeeper control over it. (You may think: “Of
course!” — but imagine being told that you can’t use Safari to access
any Google services without Apple’s explicit consent and approval.)
When you buy a Smart Thing, you get locked into its software ecosystem,
which is controlled by its manufacturer, whether you like it or not.

Techno-utopians like to argue that open systems always win, but that
simply isn’t true, as the mobile era has shown. Android is more open
than iOS, but for most intents and purposes, both are walled gardens.

The term Internet-of-things lulls us into complacency. What if we
called it what corporations actually want eg the
Apple-Store-of-things™?

— Scott Jenson (@scottjenson) September 24, 2014

So are we doomed to a future of fifth-column Smart Things that we don’t
really own, talking behind our backs to an array of siloed Stacks?

…Maybe. But not necessarily.

For one thing, I suspect that at some point, after the first wave of
the Internet of Things, open APIs and root access will become a selling
point. Either enough customers (especially business customers) will
want them badly enough, or smart hardware will become enough of a
commodity that startups will start selling “repluggable” Smart Things,
which buyers can root and configure to speak to the server(s) of their
choice.

More interesting to me, though, is the possibility of a decentralized
Internet of Things; smart things which don’t communicate with any
central server, but rather with a peer-to-peer, perhaps
blockchain-based network. Consider the way FireChat is being used in
Hong Kong, so that protestors can communicate despite the authorities’
control of the mobile networks. You don’t always actually need a
central server, especially if you have a distributed-consensus system —
like a blockchain — for longer-term data storage and algorithmic
coordination.

I concede this is a handwavey vaporware notion, but, well, I believe
it’s an important handwavey vaporware notion. Similarly, a la Overstock
or Reddit:

Every product is becoming smart. So, why not tie every product to a
corporate #blockchain that confers ownership to the customers?

— John Robb (@johnrobb) October 1, 2014

As someone who often argues that capitalism needs to evolve as
technology remakes our societies and economies, I’m not necessarily
opposed to a subtle redefinition of “ownership.” But I don’t want it to
come to mean “transferring de facto control over every interesting
thing in my possession to distant corporations.” Bring on an open,
decentralized Internet Of Things, eventually. The Stacks control quite
enough already.

How Alan Turing set the rules for computing

The Turing Machine gave the world a model for how computers could operate

Joab Jackson By Joab Jackson

U.S. Correspondent, IDG News Service
|

On Saturday, British mathematician Alan Turing would have turned 100
years old. It is barely fathomable to think that none of the computing
power surrounding us today was around when he was born.

But without Turing's work, computers as we know them today simply would
not exist, Robert Kahn, co-inventor of the TCP/IP protocols that run
the Internet, said in an interview. Absent Turing, "the computing
trajectory would have been entirely different, or at least delayed," he
said.

For while the idea of a programmable computer has been around since at
least 1837 -- when English mathematician Charles Babbage formulated the
idea of his analytical engine -- Turing was the first to do the
difficult work of mapping out the physics of how the digital universe
would operate. And he did it using a single (theoretical) strip of
infinite tape.

"Turing is so fundamental to so much of computer science that it is
hard to do anything with computers that isn't some way influenced by
his work," said Eric Brown, who was a member of the IBM team that built
the "Jeopardy"-winning Watson supercomputer.

A polymath of the highest order, Turing left a list of achievements
stretching far beyond the realm of computer science. During World War
II, he was instrumental in cracking German encrypted messages, allowing
the British to anticipate Germany's actions and ultimately help win the
war. Using his mathematical chops, he also developed ideas in the field
of non-linear biological theory, which paved the way for chaos and
complexity theories. And to a lesser extent he is known for his sad
demise, an apparent suicide after being persecuted by the British
government for his homosexuality.

But it may be computer science where his legacy will be the most
strongly felt. Last week, the Association of Computing Machinery held a
two-day celebration of Turing, with the computer field's biggest
luminaries -- Vint Cerf, Ken Thompson, Alan C. Key -- paying tribute to
the man and his work.

Turing was not alone in thinking about computers in the early part of
the past century. Mathematicians had been thinking about computable
functions for some time. Turing drew from colleagues' work at Princeton
University during the 1930s. There, Alonzo Church was defining Lambda
calculus (which later formed the basis of the Lisp programming
language). And Kurt GAPdel worked on the incompleteness theory and
recursive function theory. Turing employed the work of both
mathematicians to create a conceptual computing machine.

His 1936 paper described what would later become known as the Turing
Machine, or a-machine as he called it. In the paper, he described a
theoretical operation that used an infinitely long piece of tape
containing a series of symbols. A machine head could read the symbols
on the tape as well as add its own symbols. It could move about to
different parts of the tape, one symbol at a time.

"The Turing machine gave some ideas about what computation was, what it
would mean to have a program," said James Hendler, a professor of
computer science at the Rensselaer Polytechnic Institute and one of the
instrumental researchers of the semantic Web. "Other people were
thinking along similar lines, but Turing really put it in a formal
perspective, where you could prove things about it."

On its own, a Turing Machine could never be implemented. For one,
"infinite tapes are hard to come by," Kahn joked. But the concept
proved invaluable for the ideas it introduced into the world. "Based on
the logic of what was in the machine, Turing showed that any computable
function could be calculated," Kahn said.

Today's computers, of course, use binary logic. A computer program can
be thought of as an algorithm or set of algorithms that a compiler
converts into a series of 1's and 0's. In essence, they operate exactly
like the Turing Machine, absent the tape.

"It is generally accepted that the Turing Machine concept can be used
to model anything a digital computer can do," explained Chrisila
Pettey, who heads the Department of Computer Science at Middle
Tennessee State University.

"It's not that computer scientists sit around proving things with
Turing Machines, or even that we use Turing Machines to solve
problems," Pettey said. "It's that how Turing Machines were used to
classify problems has had a profound influence on how computer
scientists approach problem solving."

At the time Turing sketched out his ideas, the world had plenty of
pretty sophisticated adding machines that would allow someone to
perform simple calculations. What Turing offered was the idea of a
general-purpose programmable machine. "You would give it a program and
it would do what the program specified," Kahn explained.

In the next decade, another polymath, John von Neumann, at the
Princeton Institute for Advanced Study, started working on an
operational computer that borrowed from Turing's idea, except it would
use random access memory instead of infinite tape to hold the data and
operational programs. Called MANIAC (Mathematical Analyzer, Numerator,
Integrator, and Computer), it was among the first modern computers ever
built and was operational in 1952. MANIAC used what is now called the
Von Neumann architecture, the model for all computers today.

Returning to Britain after his time at Princeton, Turing worked on
another project to build a computer that used these concepts, called
the Automatic Computing Engine (ACE), and pioneered the idea of a
stored memory machine, which would become a vital part of the Von
Neumann architecture.

As well as sparking the field of computer science, the impact his work
had on cracking encryption may ultimately have also saved Great Britain
from becoming a German colony. People have argued that Turing's work
defining computers was essential to his success in breaking the
encryption generated by Germany's Enigma machine -- work that helped
bring World War II to an end.

"By today's definitions, the Enigma was an analog computer. What he
[and his team] built was much closer to [the operations] of a digital
computer," Rensselaer's Hendler explained. "Essentially he showed the
power of digital computing in attacking this analog problem. This
really changed the whole way that the field thought about what
computers could do."

Having defined computational operations, Turing went on to play a
fundamental role in defining artificial intelligence -- or computer
intelligence that mimics human thinking. In 1950, he authored a paper
that offered a way to determine if a computer possessed human
intelligence. The test involves a person having an extended
conversation with two hidden entities, a computer and a man pretending
to be a woman. ("In both cases he wanted pretending," Hendler
explained.) If the person can't determine which party is the computer,
the machine can be said to think like a human.

"He wanted to put human and computing on equal footing," Hendler said.
"Language is a critical skill for humans because it requires
understanding and context. If a computer showed that level of
understanding then you wouldn't notice the difference."

The test "has the advantage of drawing a fairly sharp line between the
physical and the intellectual capacities of a man," Turing wrote in the
original paper.

As IBM's Brown noted, Turing's legacy is still strongly felt today. In
his mathematics work, he showed that "there exists problems that no
decision process could answer," Hendler said. In terms of computers,
this means, "You could never prove for all complicated computer
programs that they are correct," Hendler said. "You could never write a
computer program that could debug all other computer programs."

But far from restricting progress of computer science, the knowledge of
such inconclusiveness paved the way for building previously unimagined
technologies. It allowed engineers to create immensely helpful services
such as Internet search engines, despite knowing that the answers such
services were to provide would not always be complete.

"You have people who say we should never build a computing system
unless we can prove it is secure. Those of us who understand Turing
say, 'Well, you can't.' So you must start proving some approximation of
secure, which starts a very different conversation," Hendler said.

And despite numerous attempts to beat the Turing Test, it still hasn't
been done, except within the most limited of topics. That means we will
likely be working to meet Turing's benchmarks for years to come.

"You can't say, 'Siri. How are you today?' and expect it to go on from
there in any interesting way," Hendler said.
